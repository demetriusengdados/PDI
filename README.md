ğŸ—ï¸ RepositÃ³rio de Arquitetura de Dados Moderna (Lakehouse, CDC e OrquestraÃ§Ã£o)
Este repositÃ³rio contÃ©m uma arquitetura de dados moderna e modular que cobre o ciclo completo de ingestÃ£o, transformaÃ§Ã£o, armazenamento, catalogaÃ§Ã£o e consumo de dados utilizando ferramentas de cÃ³digo aberto e cloud-ready.

ğŸ“š SumÃ¡rio
ğŸ“ Estrutura do Projeto

ğŸ”§ Tecnologias Utilizadas

ğŸš€ Fluxo de Dados

ğŸ“¦ Componentes do Projeto

âœ… Como Subir o Projeto

ğŸ‘¥ Contribuindo

ğŸ“ Estrutura do Projeto
.
â”œâ”€â”€ airflow/                  # OrquestraÃ§Ã£o de pipelines
â”‚   â”œâ”€â”€ dags/                # DAGs de ingestÃ£o e validaÃ§Ã£o
â”‚   â”œâ”€â”€ sensors/             # Sensores customizados (ex: KafkaSensor)
â”‚   â””â”€â”€ init.py              # InicializaÃ§Ã£o dos DAGs
â”‚
â”œâ”€â”€ beam/                    # Pipelines unificados com Apache Beam
â”‚   â””â”€â”€ unified_pipeline.py
â”‚
â”œâ”€â”€ dbt/                     # TransformaÃ§Ãµes analÃ­ticas e contratos de dados
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ kafka/                   # ConfiguraÃ§Ãµes de Kafka + Debezium
â”‚   â”œâ”€â”€ debezium-config/     # ConfiguraÃ§Ã£o de captura de dados (CDC)
â”‚   â””â”€â”€ sink-connectors/     # Connectors para S3, BigQuery, Snowflake
â”‚
â”œâ”€â”€ lakehouse/
â”‚   â”œâ”€â”€ delta/               # Pipelines de escrita Delta Lake
â”‚   â”œâ”€â”€ hudi/                # Pipelines de escrita Hudi
â”‚   â””â”€â”€ iceberg/            # Pipelines de escrita Iceberg
â”‚
â”œâ”€â”€ datahub/                 # Metadata, Lineage e GovernanÃ§a
â”‚   â””â”€â”€ ingestion/
â”‚       â””â”€â”€ datahub-config.yml
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ databricks/          # Notebooks para Databricks
â”‚   â””â”€â”€ emr/                 # Notebooks para Spark EMR
â”‚
â”œâ”€â”€ terraform/               # Provisionamento de infraestrutura em nuvem
â”‚   â”œâ”€â”€ aws/
â”‚   â”œâ”€â”€ azure/
â”‚   â””â”€â”€ gcp/
â”‚
â”œâ”€â”€ schemas/                 # Contratos de dados em JSON Schema
â”‚   â”œâ”€â”€ produtos.schema.json
â”‚   â””â”€â”€ transacoes.schema.json
â”‚
â”œâ”€â”€ .env.example             # Arquivo de exemplo para configs de ambiente
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
ğŸ”§ Tecnologias Utilizadas

Camada	Ferramentas
IngestÃ£o	Kafka, Debezium, Kafka Connect
Processamento	Apache Spark, Apache Beam, Apache Flink
Armazenamento	Delta Lake, Apache Hudi, Apache Iceberg
OrquestraÃ§Ã£o	Apache Airflow
TransformaÃ§Ãµes SQL	DBT
Metadata	DataHub
Infraestrutura	Terraform
Notebook Interface	Databricks, EMR Notebooks
Contratos de Dados	JSON Schema, Data Contracts
ğŸš€ Fluxo de Dados
CDC com Debezium: captura alteraÃ§Ãµes de tabelas PostgreSQL/MySQL â†’ envia para Kafka.

Kafka Sensor no Airflow: detecta eventos e dispara DAGs.

DAGs no Airflow: validam schema, salvam staging em Delta/Hudi/Iceberg.

Apache Beam: realiza enriquecimento e escreve no destino final (ex: BigQuery).

DBT: aplica regras de negÃ³cio, modelagem em camadas (staging â†’ mart).

DataHub: cataloga datasets, contratos e lineage.

Infra via Terraform: tudo provisionado para AWS, Azure ou GCP.

ğŸ“¦ Componentes do Projeto
ğŸ§© Airflow
init.py: inicializa DAGs com o contexto correto.

kafka_sensor.py: sensor customizado que escuta tÃ³picos Kafka.

DAGs: ingestao_produtos.py, validacao_schema.py, etc.

âš™ï¸ Kafka Connect
debezium-postgres-connector.json: captura CDC.

s3-sink-connector.json, bigquery-sink-connector.json: entrega em diferentes camadas de destino.

ğŸ“Š DBT
Modelos: stg_produtos.sql, dim_produto.sql

Macros: funÃ§Ãµes SQL reutilizÃ¡veis.

Tests: unique, not_null, relationship, etc.

ğŸ§ª Beam Pipeline
unified_pipeline.py: leitura do Kafka, enriquecimento e gravaÃ§Ã£o.

ğŸ§¬ DataHub
datahub-config.yml: define ingestÃ£o de metadados para DataHub.

â˜ï¸ Terraform
CriaÃ§Ã£o de buckets S3, clusters EMR, instÃ¢ncias Kafka e permissÃµes.

âœ… Como Subir o Projeto
Configurar ambiente
cp .env.example .env

# preencha suas variÃ¡veis (DB, Kafka, Cloud, etc)
Subir Kafka + Debezium (local)
docker-compose -f kafka/docker-compose.yml up -d

Rodar Airflow localmente
cd airflow
docker-compose up

Deploy DataHub
datahub docker quickstart

Rodar notebooks (Databricks ou EMR)
Use os notebooks na pasta notebooks/databricks ou notebooks/emr

Executar Terraform
cd terraform/aws
terraform init && terraform apply

# ğŸŒ Projeto IoT Industrial com Profinet, MQTT e Armazenamento em Banco

Este projeto simula um ambiente de automaÃ§Ã£o industrial com sensores conectados a uma rede **Profinet**, enviando dados para a **nuvem** via **MQTT**, e armazenando esses dados em um banco de dados local **SQLite**.

---

## ğŸ”§ Tecnologias e Bibliotecas Usadas

- Python 3.10+
- `paho-mqtt` â€“ Envio/recebimento de dados via MQTT
- `sqlite3` â€“ Armazenamento local dos dados IoT
- `json`, `datetime`, `time` â€“ Processamento e formataÃ§Ã£o
- Broker pÃºblico: [HiveMQ](https://broker.hivemq.com)

---

## ğŸ“‚ Estrutura

ğŸ“ projeto-iot-industrial/ â”œâ”€â”€ dispositivo_iot.py # Simula dados e envia para MQTT â”œâ”€â”€ nuvem_com_banco.py # Recebe dados MQTT e salva no SQLite â”œâ”€â”€ dados_iot.db # Banco de dados gerado â”œâ”€â”€ requirements.txt # DependÃªncias (paho-mqtt) â””â”€â”€ README.md

---

## â–¶ï¸ Como executar

### 1. Instale as dependÃªncias:
```bash
pip install paho-mqtt

Execute o script de nuvem (servidor):
python nuvem_com_banco.py

Em outro terminal, execute o script do dispositivo simulador:
python dispositivo_iot.py

ğŸ’¾ Banco de dados
Os dados sÃ£o armazenados em dados_iot.db com a seguinte estrutura:

id	temperatura	umidade	timestamp_dispositivo	recebido_na_nuvem

ğŸ”œ PrÃ³ximos passos
Dashboard com Streamlit para visualizaÃ§Ã£o em tempo real

Alertas por e-mail/Telegram

MigraÃ§Ã£o para banco de dados em nuvem (PostgreSQL, MongoDB, etc.)

IntegraÃ§Ã£o com Azure IoT Hub e AWS IoT Core


ğŸ¤ ContribuiÃ§Ã£o
Pull requests sÃ£o bem-vindos! Sinta-se Ã  vontade para colaborar.

Fork o repositÃ³rio

Crie sua branch com sua feature: git checkout -b minha-feature

Commit suas mudanÃ§as: git commit -m 'feat: adiciona nova DAG de ingestÃ£o'

Push para a branch: git push origin minha-feature

Crie um Pull Request

ğŸ“« Contato
Demetrius Magela da Mata - archdataconsultoria@gmail.com