ğŸ—ï¸ RepositÃ³rio de Arquitetura de Dados Moderna (Lakehouse, CDC e OrquestraÃ§Ã£o)
Este repositÃ³rio contÃ©m uma arquitetura de dados moderna e modular que cobre o ciclo completo de ingestÃ£o, transformaÃ§Ã£o, armazenamento, catalogaÃ§Ã£o e consumo de dados utilizando ferramentas de cÃ³digo aberto e cloud-ready.

ğŸ“š SumÃ¡rio
ğŸ“ Estrutura do Projeto

ğŸ”§ Tecnologias Utilizadas

ğŸš€ Fluxo de Dados

ğŸ“¦ Componentes do Projeto

âœ… Como Subir o Projeto

ğŸ‘¥ Contribuindo

ğŸ“ Estrutura do Projeto
.
â”œâ”€â”€ airflow/                  # OrquestraÃ§Ã£o de pipelines
â”‚   â”œâ”€â”€ dags/                # DAGs de ingestÃ£o e validaÃ§Ã£o
â”‚   â”œâ”€â”€ sensors/             # Sensores customizados (ex: KafkaSensor)
â”‚   â””â”€â”€ init.py              # InicializaÃ§Ã£o dos DAGs
â”‚
â”œâ”€â”€ beam/                    # Pipelines unificados com Apache Beam
â”‚   â””â”€â”€ unified_pipeline.py
â”‚
â”œâ”€â”€ dbt/                     # TransformaÃ§Ãµes analÃ­ticas e contratos de dados
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ kafka/                   # ConfiguraÃ§Ãµes de Kafka + Debezium
â”‚   â”œâ”€â”€ debezium-config/     # ConfiguraÃ§Ã£o de captura de dados (CDC)
â”‚   â””â”€â”€ sink-connectors/     # Connectors para S3, BigQuery, Snowflake
â”‚
â”œâ”€â”€ lakehouse/
â”‚   â”œâ”€â”€ delta/               # Pipelines de escrita Delta Lake
â”‚   â”œâ”€â”€ hudi/                # Pipelines de escrita Hudi
â”‚   â””â”€â”€ iceberg/            # Pipelines de escrita Iceberg
â”‚
â”œâ”€â”€ datahub/                 # Metadata, Lineage e GovernanÃ§a
â”‚   â””â”€â”€ ingestion/
â”‚       â””â”€â”€ datahub-config.yml
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ databricks/          # Notebooks para Databricks
â”‚   â””â”€â”€ emr/                 # Notebooks para Spark EMR
â”‚
â”œâ”€â”€ terraform/               # Provisionamento de infraestrutura em nuvem
â”‚   â”œâ”€â”€ aws/
â”‚   â”œâ”€â”€ azure/
â”‚   â””â”€â”€ gcp/
â”‚
â”œâ”€â”€ schemas/                 # Contratos de dados em JSON Schema
â”‚   â”œâ”€â”€ produtos.schema.json
â”‚   â””â”€â”€ transacoes.schema.json
â”‚
â”œâ”€â”€ .env.example             # Arquivo de exemplo para configs de ambiente
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
ğŸ”§ Tecnologias Utilizadas

Camada	Ferramentas
IngestÃ£o	Kafka, Debezium, Kafka Connect
Processamento	Apache Spark, Apache Beam, Apache Flink
Armazenamento	Delta Lake, Apache Hudi, Apache Iceberg
OrquestraÃ§Ã£o	Apache Airflow
TransformaÃ§Ãµes SQL	DBT
Metadata	DataHub
Infraestrutura	Terraform
Notebook Interface	Databricks, EMR Notebooks
Contratos de Dados	JSON Schema, Data Contracts
ğŸš€ Fluxo de Dados
CDC com Debezium: captura alteraÃ§Ãµes de tabelas PostgreSQL/MySQL â†’ envia para Kafka.

Kafka Sensor no Airflow: detecta eventos e dispara DAGs.

DAGs no Airflow: validam schema, salvam staging em Delta/Hudi/Iceberg.

Apache Beam: realiza enriquecimento e escreve no destino final (ex: BigQuery).

DBT: aplica regras de negÃ³cio, modelagem em camadas (staging â†’ mart).

DataHub: cataloga datasets, contratos e lineage.

Infra via Terraform: tudo provisionado para AWS, Azure ou GCP.

ğŸ“¦ Componentes do Projeto
ğŸ§© Airflow
init.py: inicializa DAGs com o contexto correto.

kafka_sensor.py: sensor customizado que escuta tÃ³picos Kafka.

DAGs: ingestao_produtos.py, validacao_schema.py, etc.

âš™ï¸ Kafka Connect
debezium-postgres-connector.json: captura CDC.

s3-sink-connector.json, bigquery-sink-connector.json: entrega em diferentes camadas de destino.

ğŸ“Š DBT
Modelos: stg_produtos.sql, dim_produto.sql

Macros: funÃ§Ãµes SQL reutilizÃ¡veis.

Tests: unique, not_null, relationship, etc.

ğŸ§ª Beam Pipeline
unified_pipeline.py: leitura do Kafka, enriquecimento e gravaÃ§Ã£o.

ğŸ§¬ DataHub
datahub-config.yml: define ingestÃ£o de metadados para DataHub.

â˜ï¸ Terraform
CriaÃ§Ã£o de buckets S3, clusters EMR, instÃ¢ncias Kafka e permissÃµes.

âœ… Como Subir o Projeto
Configurar ambiente
cp .env.example .env

# preencha suas variÃ¡veis (DB, Kafka, Cloud, etc)
Subir Kafka + Debezium (local)
docker-compose -f kafka/docker-compose.yml up -d

Rodar Airflow localmente
cd airflow
docker-compose up

Deploy DataHub
datahub docker quickstart

Rodar notebooks (Databricks ou EMR)
Use os notebooks na pasta notebooks/databricks ou notebooks/emr

Executar Terraform
cd terraform/aws
terraform init && terraform apply

ğŸ‘¥ Contribuindo
Fork o repositÃ³rio

Crie sua branch com sua feature: git checkout -b minha-feature

Commit suas mudanÃ§as: git commit -m 'feat: adiciona nova DAG de ingestÃ£o'

Push para a branch: git push origin minha-feature

Crie um Pull Request